{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammad-Daniyal-Imtiaz/Assignment/blob/main/Untitled18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8Qu5cDnyOwc",
        "outputId": "096632fa-a9dd-4284-bddd-0502635d96b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Drawn Matches: 0\n",
            "Number of Canceled Matches (No Result): 0\n",
            "Number of Super Over Matches: 0\n",
            "Files with missing or invalid data:\n",
            "  - /content/.config/.last_opt_in_prompt.yaml\n",
            "  - /content/.config/.last_survey_prompt.yaml\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import yaml\n",
        "\n",
        "# Path to dataset\n",
        "dataset_path = '/content'\n",
        "\n",
        "# Initialize counters\n",
        "draw_count = 0\n",
        "canceled_count = 0\n",
        "super_over_count = 0\n",
        "invalid_files = []\n",
        "\n",
        "# Function to check the outcome of matches\n",
        "def inspect_match_outcome(file_path):\n",
        "    global draw_count, canceled_count, super_over_count, invalid_files\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            data = yaml.safe_load(file)\n",
        "            if not data or 'info' not in data:\n",
        "                invalid_files.append(file_path)\n",
        "                return\n",
        "\n",
        "            outcome = data['info'].get('outcome', {})\n",
        "            if not outcome:\n",
        "                invalid_files.append(file_path)\n",
        "                return\n",
        "\n",
        "            # Check for draw\n",
        "            if outcome.get('result') == 'draw':\n",
        "                draw_count += 1\n",
        "\n",
        "            # Check for canceled matches (no result)\n",
        "            elif outcome.get('result') == 'no result':\n",
        "                canceled_count += 1\n",
        "\n",
        "            # Check for super over matches\n",
        "            if 'super_over' in outcome and outcome['super_over']:\n",
        "                super_over_count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        invalid_files.append(file_path)\n",
        "\n",
        "# Iterate over YAML files\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.yaml'):\n",
        "            file_path = os.path.join(root, file)\n",
        "            inspect_match_outcome(file_path)\n",
        "\n",
        "# Print results\n",
        "print(f\"Number of Drawn Matches: {draw_count}\")\n",
        "print(f\"Number of Canceled Matches (No Result): {canceled_count}\")\n",
        "print(f\"Number of Super Over Matches: {super_over_count}\")\n",
        "if invalid_files:\n",
        "    print(\"Files with missing or invalid data:\")\n",
        "    for invalid_file in invalid_files:\n",
        "        print(f\"  - {invalid_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd2ODboPZqik",
        "outputId": "5b6bb20a-ade7-4233-b5c3-10c8f126118d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Innings Total (Team 1): 178\n",
            "Second Innings Total (Team 2): 151\n",
            "Overs Played by First Team: 20\n",
            "Overs Played by Second Team: 20\n",
            "First Team Current Run Rate: 8.9\n",
            "Second Team Current Run Rate: 7.55\n",
            "Wickets Left for First Team: 5\n",
            "Wickets Left for Second Team: 2\n",
            "Runs Scored in Last 5 Overs (First Team): 66\n",
            "Runs Scored in Last 5 Overs (Second Team): 31\n",
            "Match Venue: Sharjah Cricket Stadium\n",
            "Match City: Stadium\n",
            "Net Run Rate (First Team): 1.35\n",
            "Net Run Rate (Second Team): -1.35\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "\n",
        "# Load the YAML data from the file\n",
        "with open('959197.yaml', 'r') as file:\n",
        "    data = yaml.safe_load(file)\n",
        "\n",
        "# Extract first-innings and second-innings deliveries\n",
        "first_innings = data['innings'][0]['1st innings']['deliveries']\n",
        "second_innings = data['innings'][1]['2nd innings']['deliveries']\n",
        "\n",
        "# Calculate the first-innings total score\n",
        "first_innings_total = sum(delivery[next(iter(delivery))]['runs']['total'] for delivery in first_innings)\n",
        "\n",
        "# Calculate the second-innings total score\n",
        "second_innings_total = sum(delivery[next(iter(delivery))]['runs']['total'] for delivery in second_innings)\n",
        "\n",
        "# Calculate overs for both innings by counting the deliveries\n",
        "overs_first_innings = len(first_innings) / 6  # 6 balls = 1 over\n",
        "overs_second_innings = len(second_innings) / 6  # 6 balls = 1 over\n",
        "\n",
        "# Ensure overs do not exceed the maximum for a T20 match (20 overs)\n",
        "overs_first_innings = min(overs_first_innings, 20)\n",
        "overs_second_innings = min(overs_second_innings, 20)\n",
        "\n",
        "# Calculate current run rate for the first team\n",
        "current_run_rate_first_team = first_innings_total / overs_first_innings\n",
        "\n",
        "# Calculate current run rate for the second team\n",
        "current_run_rate_second_team = second_innings_total / overs_second_innings\n",
        "\n",
        "# Extract wickets left for the first team\n",
        "wickets_left_first_team = 10 - len([delivery for delivery in first_innings if 'wicket' in delivery[next(iter(delivery))]])\n",
        "\n",
        "# Extract wickets left for the second team\n",
        "wickets_left_second_team = 10 - len([delivery for delivery in second_innings if 'wicket' in delivery[next(iter(delivery))]])\n",
        "\n",
        "# Calculate runs scored in the last five overs for the first team\n",
        "last_five_overs_first_team = first_innings[-30:]  # Last 30 deliveries (5 overs)\n",
        "runs_last_five_overs_first_team = sum(delivery[next(iter(delivery))]['runs']['total'] for delivery in last_five_overs_first_team)\n",
        "\n",
        "# Calculate runs scored in the last five overs for the second team\n",
        "last_five_overs_second_team = second_innings[-30:]  # Last 30 deliveries (5 overs)\n",
        "runs_last_five_overs_second_team = sum(delivery[next(iter(delivery))]['runs']['total'] for delivery in last_five_overs_second_team)\n",
        "\n",
        "# Extract match venue and city\n",
        "venue = data['info']['venue']\n",
        "city = venue.split()[-1] if 'venue' in data['info'] else \"Unknown\"\n",
        "\n",
        "# Calculate Net Run Rate (NRR)\n",
        "total_runs_scored = first_innings_total + second_innings_total\n",
        "total_overs_faced = overs_first_innings + overs_second_innings\n",
        "\n",
        "# Calculate runs conceded by first team (second innings total) and overs bowled by first team (second innings overs)\n",
        "runs_conceded_first_team = second_innings_total\n",
        "overs_bowled_first_team = overs_second_innings\n",
        "\n",
        "# Calculate runs conceded by second team (first innings total) and overs bowled by second team (first innings overs)\n",
        "runs_conceded_second_team = first_innings_total\n",
        "overs_bowled_second_team = overs_first_innings\n",
        "\n",
        "# NRR for both teams\n",
        "nrr_first_team = (first_innings_total / overs_first_innings) - (runs_conceded_first_team / overs_bowled_first_team)\n",
        "nrr_second_team = (second_innings_total / overs_second_innings) - (runs_conceded_second_team / overs_bowled_second_team)\n",
        "\n",
        "# Display the results\n",
        "print(\"First Innings Total (Team 1):\", first_innings_total)\n",
        "print(\"Second Innings Total (Team 2):\", second_innings_total)\n",
        "print(\"Overs Played by First Team:\", round(overs_first_innings, 2))\n",
        "print(\"Overs Played by Second Team:\", round(overs_second_innings, 2))\n",
        "print(\"First Team Current Run Rate:\", round(current_run_rate_first_team, 2))\n",
        "print(\"Second Team Current Run Rate:\", round(current_run_rate_second_team, 2))\n",
        "print(\"Wickets Left for First Team:\", wickets_left_first_team)\n",
        "print(\"Wickets Left for Second Team:\", wickets_left_second_team)\n",
        "print(\"Runs Scored in Last 5 Overs (First Team):\", runs_last_five_overs_first_team)\n",
        "print(\"Runs Scored in Last 5 Overs (Second Team):\", runs_last_five_overs_second_team)\n",
        "print(\"Match Venue:\", venue)\n",
        "print(\"Match City:\", city)\n",
        "print(\"Net Run Rate (First Team):\", round(nrr_first_team, 2))\n",
        "print(\"Net Run Rate (Second Team):\", round(nrr_second_team, 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrtdEasDr_NT",
        "outputId": "3525be66-8d51-4ab1-f928-8c78cc3d4db9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn==1.1.3 in /usr/local/lib/python3.10/dist-packages (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.3) (3.5.0)\n",
            "Best Parameters: {'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 150, 'subsample': 1.0}\n",
            "Mean Absolute Error: 0.8980610147816339\n",
            "R-squared: 0.9884068332631359\n",
            "Predicted Future Runs for Sample Data: 62.24773\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn==1.1.3\n",
        "\n",
        "import yaml\n",
        "import os\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import warnings\n",
        "\n",
        "# Ignore warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Directory containing the YAML files\n",
        "yaml_directory = '/content'\n",
        "\n",
        "# List to hold the data for training\n",
        "data_list = []\n",
        "\n",
        "# Helper function to calculate overs and balls\n",
        "def calculate_overs(deliveries):\n",
        "    total_balls = 0\n",
        "    for delivery in deliveries:\n",
        "        if 'extras' not in delivery[next(iter(delivery))] or (\n",
        "            delivery[next(iter(delivery))]['extras'].get('wides', 0) == 0 and\n",
        "            delivery[next(iter(delivery))]['extras'].get('noballs', 0) == 0\n",
        "        ):\n",
        "            total_balls += 1\n",
        "    overs = total_balls // 6\n",
        "    balls = total_balls % 6\n",
        "    return overs, balls\n",
        "\n",
        "# Function to process each YAML file\n",
        "def process_yaml_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = yaml.safe_load(file)\n",
        "\n",
        "    try:\n",
        "        first_innings = data['innings'][0]['1st innings']['deliveries']\n",
        "        second_innings = data['innings'][1]['2nd innings']['deliveries']\n",
        "    except IndexError:\n",
        "        return  # Skip files without complete innings\n",
        "\n",
        "    # Process features for both innings\n",
        "    for innings, innings_name in zip([first_innings, second_innings], ['1st innings', '2nd innings']):\n",
        "        if innings:\n",
        "            total_runs = sum(delivery[next(iter(delivery))]['runs']['total'] for delivery in innings)\n",
        "            overs, balls = calculate_overs(innings)\n",
        "            balls_left = 120 - len(innings)\n",
        "            wickets_left = 10 - len([delivery for delivery in innings if 'wicket' in delivery[next(iter(delivery))]])\n",
        "\n",
        "            # Calculate additional features\n",
        "            run_rate = total_runs / overs if overs > 0 else 0\n",
        "            runs_last_5_overs = sum(\n",
        "                delivery[next(iter(delivery))]['runs']['total']\n",
        "                for delivery in innings[-30:]\n",
        "            )\n",
        "            wickets_impact = 1 + (wickets_left * 0.1)\n",
        "\n",
        "            # Define future runs with rules for edge cases\n",
        "            if wickets_left == 0:  # All out\n",
        "                future_runs = 0\n",
        "            elif balls_left < 15:  # Very few balls left\n",
        "                overs_left = balls_left / 6\n",
        "                future_runs = run_rate * overs_left\n",
        "            else:  # General case\n",
        "                future_runs = (run_rate * (balls_left / 6)) * wickets_impact + runs_last_5_overs * 0.1\n",
        "\n",
        "            # Append features to the list\n",
        "            data_list.append({\n",
        "                'team_name': data['info']['teams'][0] if innings_name == '1st innings' else data['info']['teams'][1],\n",
        "                'current_run_rate': run_rate,\n",
        "                'wickets_left': wickets_left,\n",
        "                'runs_last_5_overs': runs_last_5_overs,\n",
        "                'balls_left': balls_left,\n",
        "                'current_score': total_runs,\n",
        "                'wickets_impact': wickets_impact,\n",
        "                'future_runs': future_runs  # Label\n",
        "            })\n",
        "\n",
        "# Process all YAML files in the directory\n",
        "for filename in os.listdir(yaml_directory):\n",
        "    if filename.endswith('.yaml'):\n",
        "        process_yaml_file(os.path.join(yaml_directory, filename))\n",
        "\n",
        "# Convert the collected data to a pandas DataFrame\n",
        "df = pd.DataFrame(data_list)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X = df[['current_run_rate', 'wickets_left', 'runs_last_5_overs', 'balls_left', 'current_score', 'wickets_impact']]\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "y = df['future_runs']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Hyperparameter tuning with GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb.XGBRegressor(),\n",
        "    param_grid=param_grid,\n",
        "    cv=kf,\n",
        "    n_jobs=-1,\n",
        "    scoring='neg_mean_absolute_error'\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model and evaluation\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "print(\"R-squared:\", r2)\n",
        "\n",
        "# Testing on sample data\n",
        "sample_data = pd.DataFrame({\n",
        "    'current_run_rate': [7.5],\n",
        "    'wickets_left': [8],\n",
        "    'runs_last_5_overs': [40],\n",
        "    'balls_left': [30],\n",
        "    'current_score': [150],\n",
        "    'wickets_impact': [1.3]\n",
        "})\n",
        "sample_scaled = scaler.transform(sample_data)\n",
        "future_runs_prediction = best_model.predict(sample_scaled)\n",
        "\n",
        "print(\"Predicted Future Runs for Sample Data:\", future_runs_prediction[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "PPPcxlZNtH-0",
        "outputId": "e94d4023-100c-4e15-e553-0059912db3b9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "X has 6 features, but ColumnTransformer is expecting 9 features as input.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d65748b08119>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m })\n\u001b[1;32m      9\u001b[0m \u001b[0msample_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfuture_runs_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Future Runs for Sample Data:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture_runs_prediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;31m# ndarray was used for fitting or transforming, thus we only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;31m# check that n_features_in_ is consistent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         Xs = self._fit_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    401\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 6 features, but ColumnTransformer is expecting 9 features as input."
          ]
        }
      ],
      "source": [
        "sample_data = pd.DataFrame({\n",
        "    'current_run_rate': [5.5],\n",
        "    'wickets_left': [5],\n",
        "    'runs_last_5_overs': [40],\n",
        "    'balls_left': [18],\n",
        "    'current_score': [150],\n",
        "    'wickets_impact': [1.3]\n",
        "})\n",
        "sample_scaled = scaler.transform(sample_data)\n",
        "future_runs_prediction = best_model.predict(sample_scaled)\n",
        "\n",
        "print(\"Predicted Future Runs for Sample Data:\", future_runs_prediction[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD9qNNcHWeqY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0f8c5-ahKan",
        "outputId": "399831ae-0e00-4da9-8b22-845c25405add"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Future Runs: 143.10215759277344\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR5Eok2LgH51"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2EGJ-GCw3Xc",
        "outputId": "9dabfd7f-622d-44a8-d0b9-118c5a833bad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 278 matches.\n",
            "Best Parameters: {'model__learning_rate': 0.01, 'model__max_depth': 3, 'model__n_estimators': 50, 'model__subsample': 1.0}\n",
            "Accuracy on Test Data: 1.0\n",
            "Predicted Winner: Batting First\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "import os\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Directory containing the YAML files\n",
        "yaml_directory = '/content'\n",
        "\n",
        "# List to hold the data for training\n",
        "data_list = []\n",
        "\n",
        "# Helper function to process innings data\n",
        "def process_innings(innings, first_innings_runs, venue, city, team_batting_second):\n",
        "    total_runs = sum(delivery[next(iter(delivery))]['runs']['total'] for delivery in innings)\n",
        "    balls_faced = len(innings)\n",
        "\n",
        "    # Calculate current run rate (CRR)\n",
        "    current_run_rate = (total_runs / balls_faced) * 6 if balls_faced > 0 else 0\n",
        "\n",
        "    # Remaining balls and overs\n",
        "    remaining_balls = 120 - balls_faced\n",
        "    overs_left = remaining_balls / 6\n",
        "\n",
        "    # Runs left to chase and required run rate\n",
        "    runs_left_to_chase = first_innings_runs - total_runs\n",
        "    required_run_rate = runs_left_to_chase / overs_left if overs_left > 0 else 0\n",
        "\n",
        "    # Decision-making: if runs left to chase is <= 0, team batting second wins\n",
        "    match_winner = team_batting_second if runs_left_to_chase <= 0 else 'Batting First'\n",
        "\n",
        "    return {\n",
        "        'runs_left_to_chase': runs_left_to_chase,\n",
        "        'required_run_rate': required_run_rate,\n",
        "        'current_run_rate': current_run_rate,\n",
        "        'remaining_balls': remaining_balls,\n",
        "        'overs_left': overs_left,\n",
        "        'first_innings_total': first_innings_runs,\n",
        "        'venue': venue,\n",
        "        'city': city,\n",
        "        'team_batting_second': team_batting_second,\n",
        "        'match_winner': match_winner\n",
        "    }\n",
        "\n",
        "# Process all YAML files in the directory\n",
        "for filename in os.listdir(yaml_directory):\n",
        "    if filename.endswith('.yaml'):\n",
        "        try:\n",
        "            with open(os.path.join(yaml_directory, filename), 'r') as file:\n",
        "                data = yaml.safe_load(file)\n",
        "\n",
        "            # Attempt to extract data, with error handling for missing keys\n",
        "            try:\n",
        "                first_innings = data['innings'][0]['1st innings']['deliveries']\n",
        "                second_innings = data['innings'][1]['2nd innings']['deliveries']\n",
        "                venue = data['info'].get('venue', 'Unknown Venue')\n",
        "                city = data['info'].get('city', 'Unknown City')\n",
        "                team_batting_second = data['info']['teams'][1]\n",
        "            except (KeyError, IndexError) as e:\n",
        "                print(f\"Skipping {filename} due to missing data: {e}\")\n",
        "                continue  # Skip files with incomplete or missing data\n",
        "\n",
        "            first_innings_runs = sum(delivery[next(iter(delivery))]['runs']['total'] for delivery in first_innings)\n",
        "\n",
        "            # Append processed data for second innings\n",
        "            match_data = process_innings(\n",
        "                innings=second_innings,\n",
        "                first_innings_runs=first_innings_runs,\n",
        "                venue=venue,\n",
        "                city=city,\n",
        "                team_batting_second=team_batting_second\n",
        "            )\n",
        "            data_list.append(match_data)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {filename}: {e}\")\n",
        "            continue  # Skip problematic files\n",
        "\n",
        "# Convert the collected data to a pandas DataFrame\n",
        "df = pd.DataFrame(data_list)\n",
        "\n",
        "# Check if data is collected successfully\n",
        "if df.empty:\n",
        "    print(\"No valid data was processed.\")\n",
        "else:\n",
        "    print(f\"Successfully processed {len(df)} matches.\")\n",
        "\n",
        "    # Preprocessing\n",
        "    categorical_features = ['venue', 'city', 'team_batting_second']\n",
        "    numerical_features = ['runs_left_to_chase', 'required_run_rate', 'current_run_rate', 'remaining_balls', 'overs_left', 'first_innings_total']\n",
        "    target = 'match_winner'\n",
        "\n",
        "    # Label encoding for target\n",
        "    label_encoder = LabelEncoder()\n",
        "    df[target] = label_encoder.fit_transform(df[target])\n",
        "\n",
        "    # Split data\n",
        "    X = df[categorical_features + numerical_features]\n",
        "    y = df[target]\n",
        "\n",
        "    # One-hot encoding and scaling\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', Pipeline(steps=[\n",
        "                ('imputer', SimpleImputer(strategy='mean')),\n",
        "                ('scaler', StandardScaler())\n",
        "            ]), numerical_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Model pipeline\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
        "    ])\n",
        "\n",
        "    # Hyperparameter tuning\n",
        "    param_grid = {\n",
        "        'model__n_estimators': [50, 100, 150],\n",
        "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "        'model__max_depth': [3, 5, 7],\n",
        "        'model__subsample': [0.8, 1.0]\n",
        "    }\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=pipeline,\n",
        "        param_grid=param_grid,\n",
        "        cv=kf,\n",
        "        n_jobs=-1,  # Use all available cores\n",
        "        scoring='accuracy'\n",
        "    )\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Fit the model\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Best model evaluation\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(\"Best Parameters:\", grid_search.best_params_)\n",
        "    print(\"Accuracy on Test Data:\", accuracy)\n",
        "\n",
        "    # Test with sample data\n",
        "    sample_data = pd.DataFrame({\n",
        "        'venue': ['National Stadium'],\n",
        "        'city': ['Karachi'],\n",
        "        'team_batting_second': ['Team B'],\n",
        "        'runs_left_to_chase': [100],\n",
        "        'required_run_rate': [10.0],\n",
        "        'current_run_rate': [7.5],\n",
        "        'remaining_balls': [30],\n",
        "        'overs_left': [5.0],\n",
        "        'first_innings_total': [180]\n",
        "    })\n",
        "\n",
        "    sample_pred = best_model.predict(sample_data)\n",
        "    predicted_winner = label_encoder.inverse_transform(sample_pred)\n",
        "    print(\"Predicted Winner:\", predicted_winner[0])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBlJz8/aTl92sE/IkE1DCh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}